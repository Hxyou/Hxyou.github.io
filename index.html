
    <!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A==" crossorigin="anonymous" referrerpolicy="no-referrer" />

  <title>Haoxuan You @ Columbia University</title>
  <link rel="icon" type="image/x-icon" href="assets/columbia.png">
</head>

<body>
    <div class="container">
        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="margin-bottom: 1em;">
            <h3 class="display-4" style="text-align: center;"><span style="font-weight: bold;">Haoxuan You </span> <span STYLE="font-size:43px; font-family:'Microsoft YaHei'">(æœ‰æ˜Šè½©) </span> </h3>
            </div>
            <br>
            <div class="col-md-8" style="">
                
                <p>
                    <!-- # <span style="font-weight: bold;">Bio:</span>  -->
                    I am a fifth-year Computer Science PhD student at the Columbia University, advised by <a href="https://www.ee.columbia.edu/~sfchang/" target="_blank">Prof. Shih-Fu Chang</a> and co-advised by <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank">Prof. Kai-Wei Chang</a> from UCLA. 
                    Previously I received a Bachelor dregree from <a href="https://en.xidian.edu.cn/" target="_blank">Xidian University</a> in 2018. Then I spent a gap year working as a Research Assistant in Tsinghua University advised by <a href="https://www.gaoyue.org/" target="_blank">Prof. Yue Gao</a>,
                    in the middle of which, I visited  <a href="http://mcl.usc.edu" target="_blank">MCL lab</a> in University of Southern California, advised by <a href="http://mcl.usc.edu/people/cckuo/" target="_blank">Prof. C.-C. Jay Kuo</a>.
                </p>
                <p>
                    In my Ph.D. study, I am fortunate to intern at Microsoft Azure Cognitive Services Research (Mentor: <a href="https://luoweizhou.github.io/" target="_blank">Luowei Zhou</a>), 
                    Google Research (Mentor: <a href="https://jiahuiyu.com/" target="_blank">Jiahui Yu</a>, <a href="https://scholar.google.com/citations?user=qOiCKewAAAAJ&hl=en" target="_blank">Mandy Guo</a> and <a href="http://www.jasonbaldridge.com/" target="_blank">Jason Baldridge</a>),
                    and Apple AI/ML (Mentor: <a href="http://llcao.net/" target="_blank">Liangliang Cao</a>, <a href="https://zhegan27.github.io/" target="_blank">Zhe Gan</a> and <a href="https://sites.google.com/site/yinfeiyang" target="_blank">Yinfei Yang</a>).
                </p>

                <p>
                    <!-- I work on interesting and fundamental research problems in the field of vision-and-language, with an emphasis on scalable, unified and generalizable models/methods.   -->
                    <!-- Recently, I focus on three topics: -->
                    <!-- <ul>
                        <li><b>Vision-Language Understanding:</b> Multimodal LLM, Visual Commonsense.</li>
                        <li><b>Text-to-Image Generation:</b> Auto-regressive Text-to-Image Generation, Diffusion Model.</li>
                        <li><b>Language for Vision:</b> Language-supervised Contrastive/Generative Pre-training.</li>
                    </ul> -->
                </p>

                 <p>
                    <!-- <i><span style="color: red;"><b>News: I will be joining Apple AI/ML as a Research Scientist working on Foundation Models after graduation. </b></span></i> -->
                </p>
            
                <p>
                    <!-- <a href="https://Hxyou.github.io/assets/pdf/Phd_CV.pdf" target="_blank" style="margin-right: 15px"><i class="fa fa-address-card fa-lg"></i> CV</a> -->
                    <a href="mailto:haoxuanyou@gmail.com" style="margin-right: 15px"><i class="far fa-envelope-open fa-lg"></i> Mail</a>
                    <a href="https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en" target="_blank" style="margin-right: 15px"><i class="fa-solid fa-book"></i> Scholar</a>
                    <a href="https://github.com/Hxyou" target="_blank" style="margin-right: 15px"><i class="fab fa-github fa-lg"></i> Github</a>
                    <a href="https://www.linkedin.com/in/haoxuan-you-b9872a151" target="_blank" style="margin-right: 15px"><i class="fab fa-linkedin fa-lg"></i> LinkedIn</a>
                    <a href="https://twitter.com/XyouH" target="_blank" style="margin-right: 15px"><i class="fab fa-twitter fa-lg"></i> Twitter</a>
                </p>
    
            </div>
            <div class="col-md-4" style="">
                <figure>
                    <img src="assets/img/profile.jpeg" class="img-thumbnail" width="280px" alt="Profile picture">
                    <figcaption class="text-left">ðŸ“· credit to: my fiancÃ©e Xiaohui </figcaption>
                </figure>
            </div>
        </div>
        <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h4>Selected Publications (<a href="https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en" target="_blank">Full List</a>)</h4>
                <p>  </p>
                <div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/ferret.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2310.07704" target="_blank">Ferret: Refer and Ground Anything Anywhere at Any Granularity</a> <br><span style="font-weight: bold";>Haoxuan You*</span>, Haotian Zhang*, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, Yinfei Yang <br><span style="font-style: italic;">arXiv.org</span>, 2023 <br><a href="https://arxiv.org/abs/2310.07704" target="_blank">Project Page</a> / <a href="https://arxiv.org/abs/2310.07704.pdf" target="_blank">Paper</a> / <a href="https://github.com/apple/ml-ferret" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/idealgpt.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2305.14985" target="_blank">IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models</a> <br><span style="font-weight: bold";>Haoxuan You*</span>, Rui Sun*, Zhecan Wang*, Long Chen, Gengyu Wang, Hammad Ayyubi, Kai-Wei Chang, Shih-Fu Chang <br><span style="font-style: italic;">Empirical Methods in Natural Language Processing - Findings (EMNLP-Findings)</span>, 2023 <br><a href="https://arxiv.org/abs/2305.14985" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2305.14985.pdf" target="_blank">Paper</a> / <a href="https://github.com/Hxyou/IdealGPT" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/cobit.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2303.13455" target="_blank">CoBIT: A Contrastive Bi-directional Image-Text Generation Model</a> <br><span style="font-weight: bold";>Haoxuan You</span>, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, Jiahui Yu <br><span style="font-style: italic;">arXiv.org</span>, 2023 <br><a href="https://arxiv.org/abs/2303.13455" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2303.13455.pdf" target="_blank">Paper</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/human_cog.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2212.06971" target="_blank">Find Someone Who: Visual Commonsense Understanding in Human-Centric Grounding</a> <br><span style="font-weight: bold";>Haoxuan You</span>, Rui Sun*, Zhecan Wang*, Kai-Wei Chang, Shih-Fu Chang <br><span style="font-style: italic;">Empirical Methods in Natural Language Processing - Findings (EMNLP-Findings)</span>, 2022 <br><a href="https://arxiv.org/abs/2212.06971" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2212.06971.pdf" target="_blank">Paper</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/msclip.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2207.12661" target="_blank">Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training</a> <br><span style="font-weight: bold";>Haoxuan You*</span>, Luowei Zhou*, Bin Xiao*, Noel Codella*, Yu Cheng, Ruochen Xu, Shih-Fu Chang, Lu Yuan <br><span style="font-style: italic;">Proc. of the European Conf. on Computer Vision (ECCV)</span>, 2022 <br><a href="https://arxiv.org/abs/2207.12661" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2207.12661.pdf" target="_blank">Paper</a> / <a href="https://github.com/Hxyou/MSCLIP" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/sgeitl.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2112.08587" target="_blank">SGEITL: Scene Graph Enhanced Image-Text Learning for Visual Commonsense Reasoning</a> <br>Zhecan Wang*, <span style="font-weight: bold";>Haoxuan You*</span>, Liunian Li, Alireza Zareian, Suji Park, Yiqing Liang, Kai-Wei Chang, Shih-Fu Chang <br><span style="font-style: italic;">Proc. of the AAAI Conference on Artificial Intelligence (AAAI)</span>, 2022 <br><a href="https://arxiv.org/abs/2112.08587" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2112.08587.pdf" target="_blank">Paper</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/pointmlp.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://openreview.net/forum?id=3Pbra-_u76D" target="_blank">Rethinking network design and local geometry in point cloud: A simple residual MLP framework</a> <br>Xu Ma, Can Qin, <span style="font-weight: bold";>Haoxuan You</span>, Haoxi Ran, Yun Fu <br><span style="font-style: italic;">The International Conference on Learning Representations (ICLR)</span>, 2022 <br><a href="https://openreview.net/forum?id=3Pbra-_u76D" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2202.07123.pdf" target="_blank">Paper</a> / <a href="https://github.com/ma-xu/pointMLP-pytorch" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/uvisualbert.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2010.12831" target="_blank">Unsupervised vision-and-language pre-training without parallel images and captions</a> <br>Liunian Li, <span style="font-weight: bold";>Haoxuan You*</span>, Zhecan Wang*, Alireza Zareian, Shih-Fu Chang, Kai-Wei Chang <br><span style="font-style: italic;">Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</span>, 2021 <br><a href="https://arxiv.org/abs/2010.12831" target="_blank">Project Page</a> / <a href="https://aclanthology.org/2021.naacl-main.420.pdf" target="_blank">Paper</a> / <a href="https://github.com/uclanlp/visualbert/tree/master/unsupervised_visualbert" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/glat.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/2006.09623" target="_blank">Learning Visual Commonsense for Robust Scene Graph Generation</a> <br>Alireza Zareian*, Zhecan Wang*, <span style="font-weight: bold";>Haoxuan You*</span>, Shih-Fu Chang <br><span style="font-style: italic;">Proc. of the European Conf. on Computer Vision (ECCV)</span>, 2020 <br><a href="https://arxiv.org/abs/2006.09623" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/2006.09623.pdf" target="_blank">Paper</a> / <a href="https://github.com/ZhecanJamesWang/GLAT_SGG" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/pointhop.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/1907.12766" target="_blank">PointHop: An Explainable Machine Learning Method for Point Cloud Classification</a> <br>Min Zhang, <span style="font-weight: bold";>Haoxuan You*</span>, Pranav Kadam, Shan Liu, C-C Kuo (*Corresponding Author) <br><span style="font-style: italic;">IEEE Transactions on Multimedia</span>, 2020 <br><a href="https://arxiv.org/abs/1907.12766" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/1907.12766.pdf" target="_blank">Paper</a> / <a href="https://github.com/minzhang-1/PointHop" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/pointdan.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/1911.02744" target="_blank">PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation</a> <br>Can Qin*, <span style="font-weight: bold";>Haoxuan You*</span>, Lichen Wang, C-C Kuo, Yun Fu <br><span style="font-style: italic;">Advances in Neural Information Processing Systems (NeurIPS)</span>, 2019 <br><a href="https://arxiv.org/abs/1911.02744" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/1911.02744.pdf" target="_blank">Paper</a> / <a href="https://github.com/canqin001/PointDAN" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/pvrnet.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/1812.00333" target="_blank">PVRNet: Point-View Relation Neural Network for 3D Shape Recognition</a> <br><span style="font-weight: bold";>Haoxuan You</span>, Yifan Feng, Xibin Zhao, Changqing Zou, Rongrong Ji, Yue Gao <br><span style="font-style: italic;">Proc. of the AAAI Conference on Artificial Intelligence (AAAI)</span>, 2019 <br><a href="https://arxiv.org/abs/1812.00333" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/1812.00333.pdf" target="_blank">Paper</a> / <a href="https://github.com/iMoonLab/PVRNet" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/hgnn.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/1809.09401" target="_blank">Hypergraph Neural Networks</a> <br>Yifan Feng, <span style="font-weight: bold";>Haoxuan You</span>, Zizhao Zhang, Rongrong Ji, Yue Gao <br><span style="font-style: italic;">Proc. of the AAAI Conference on Artificial Intelligence (AAAI)</span>, 2019 <br><a href="https://arxiv.org/abs/1809.09401" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/1809.09401.pdf" target="_blank">Paper</a> / <a href="https://github.com/iMoonLab/HGNN" target="_blank">Code</a> </div> </div> </div><div style="margin-bottom: 3em;"> <div class="row"><div class="col-sm-3"><img src="assets/img/publications/pvnet.png" class="img-fluid img-thumbnail" alt="Project image"></div><div class="col-sm-9"><a href="https://arxiv.org/abs/1808.07659" target="_blank">PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition</a> <br><span style="font-weight: bold";>Haoxuan You</span>, Yifan Feng, Rongrong Ji, Yue Gao <br><span style="font-style: italic;">Proc. of ACM international conference on Multimedia (ACM-MM)</span>, 2018 <br><a href="https://arxiv.org/abs/1808.07659" target="_blank">Project Page</a> / <a href="https://arxiv.org/pdf/1808.07659.pdf" target="_blank">Paper</a> </div> </div> </div>
            </div>
        </div>
        <!-- <div class="row" style="margin-top: 3em;">
            <div class="col-sm-12" style="">
                <h4>Talks</h4>
                None
            </div>
        </div>-->
        <div class="row" style="margin-top: 3em; margin-bottom: 1em;">
            
            <div class="col-sm-12" style="font-size:14px">
                <p>
                    Website template borrowed from <a href="https://m-niemeyer.github.io/" target="_blank">Michael Niemeyer</a>.
                </p>
            </div>
    
        </div>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
      integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
      crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
      integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
      crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
      integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
      crossorigin="anonymous"></script>
</body>

</html>
    